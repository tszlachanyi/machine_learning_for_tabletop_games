{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full code for running a game of tic-tac-toe on a 3 by 3 board.\n",
    "Two players take turns making moves on squares of the board, the first to get 3 in a row, including diagonals, wins. If\n",
    "there are no valid moves left to make the game ends a draw.\n",
    "The main method to use here is play_game which simulates a game to the end using the function args it takes to determine\n",
    "where each player plays.\n",
    "The board is represented by a 3 x 3 tuple of ints. A 0 means no player has played in a space, 1 means player one has\n",
    "played there, -1 means the seconds player has played there. The apply_move method can be used to return a copy of a\n",
    "given state with a given move applied. This can be useful for doing min-max or monte carlo sampling.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "player functions to use in play_game method:\n",
    "random_player - randomly chooses from available moves\n",
    "human_player - takes keyboard input to choose from the list of available moves\n",
    "min_max_player - runs the min-max algorithm to choose the best available move\n",
    "make_move - player trained with reinforced learning\n",
    "\"\"\"\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def _new_board():\n",
    "    \"\"\"Return a emprty tic-tac-toe board we can use for simulating a game.\n",
    "    Returns:\n",
    "        3x3 tuple of ints\n",
    "    \"\"\"\n",
    "    return ((0, 0, 0),\n",
    "            (0, 0, 0),\n",
    "            (0, 0, 0))\n",
    "\n",
    "\n",
    "def apply_move(board_state, move, side):\n",
    "    \"\"\"Returns a copy of the given board_state with the desired move applied.\n",
    "    Args:\n",
    "        board_state (3x3 tuple of int): The given board_state we want to apply the move to.\n",
    "        move (int, int): The position we want to make the move in.\n",
    "        side (int): The side we are making this move for, 1 for the first player, -1 for the second player.\n",
    "    Returns:\n",
    "        (3x3 tuple of int): A copy of the board_state with the given move applied for the given side.\n",
    "    \"\"\"\n",
    "    move_x, move_y = move\n",
    "\n",
    "    def get_tuples():\n",
    "        for x in range(3):\n",
    "            if move_x == x:\n",
    "                temp = list(board_state[x])\n",
    "                temp[move_y] = side\n",
    "                yield tuple(temp)\n",
    "            else:\n",
    "                yield board_state[x]\n",
    "\n",
    "    return tuple(get_tuples())\n",
    "\n",
    "\n",
    "def available_moves(board_state):\n",
    "    \"\"\"Get all legal moves for the current board_state. For Tic-tac-toe that is all positions that do not currently have\n",
    "    pieces played.\n",
    "    Args:\n",
    "        board_state: The board_state we want to check for valid moves.\n",
    "    Returns:\n",
    "        Generator of (int, int): All the valid moves that can be played in this position.\n",
    "    \"\"\"\n",
    "    for x, y in itertools.product(range(3), range(3)):\n",
    "        if board_state[x][y] == 0:\n",
    "            yield (x, y)\n",
    "\n",
    "\n",
    "def _has_3_in_a_line(line):\n",
    "    return all(x == -1 for x in line) | all(x == 1 for x in line)\n",
    "\n",
    "\n",
    "def has_winner(board_state):\n",
    "    \"\"\"Determine if a player has won on the given board_state.\n",
    "    Args:\n",
    "        board_state (3x3 tuple of int): The current board_state we want to evaluate.\n",
    "    Returns:\n",
    "        int: 1 if player one has won, -1 if player 2 has won, otherwise 0.\n",
    "    \"\"\"\n",
    "    # check rows\n",
    "    for x in range(3):\n",
    "        if _has_3_in_a_line(board_state[x]):\n",
    "            return board_state[x][0]\n",
    "    # check columns\n",
    "    for y in range(3):\n",
    "        if _has_3_in_a_line([i[y] for i in board_state]):\n",
    "            return board_state[0][y]\n",
    "\n",
    "    # check diagonals\n",
    "    if _has_3_in_a_line([board_state[i][i] for i in range(3)]):\n",
    "        return board_state[0][0]\n",
    "    if _has_3_in_a_line([board_state[2 - i][i] for i in range(3)]):\n",
    "        return board_state[0][2]\n",
    "\n",
    "    return 0  # no one has won, return 0 for a draw\n",
    "\n",
    "\n",
    "def print_board(board_state,move):\n",
    "  for i in range (len(board_state)):\n",
    "    for j in range (len(board_state[0])):\n",
    "      if(move[0]==i and move[1]==j):\n",
    "        print(\"\\033[1;33m%s\" %board_state[i][j], end = '  ')\n",
    "      else:\n",
    "        print(\"\\033[1;30m%s\" %board_state[i][j], end = '  ')\n",
    "    print(\"\\n\")\n",
    "\n",
    "def play_game(plus_player_func, minus_player_func, log=0):\n",
    "    \"\"\"Run a single game of tic-tac-toe until the end, using the provided function args to determine the moves for each\n",
    "    player.\n",
    "    Args:\n",
    "        plus_player_func ((board_state(3 by 3 tuple of int), side(int)) -> move((int, int))): Function that takes the\n",
    "            current board_state and side this player is playing, and returns the move the player wants to play.\n",
    "        minus_player_func ((board_state(3 by 3 tuple of int), side(int)) -> move((int, int))): Function that takes the\n",
    "            current board_state and side this player is playing, and returns the move the player wants to play.\n",
    "        log (int): If 1 or 2 progress is logged to console, defaults to 0\n",
    "    Returns:\n",
    "        int: 1 if the plus_player_func won, -1 if the minus_player_func won and 0 for a draw\n",
    "    \"\"\"\n",
    "    board_state = _new_board()\n",
    "    player_turn = 1\n",
    "\n",
    "    while True:\n",
    "        _available_moves = list(available_moves(board_state))\n",
    "\n",
    "        if len(_available_moves) == 0:\n",
    "            # draw\n",
    "            if log:\n",
    "                print(\"no moves left, game ended a draw\")\n",
    "            return 0.\n",
    "        if player_turn > 0:\n",
    "            move = plus_player_func(board_state, 1)\n",
    "        else:\n",
    "            move = minus_player_func(board_state, -1)\n",
    "\n",
    "        if move not in _available_moves:\n",
    "            # if a player makes an invalid move the other player wins\n",
    "            if log:\n",
    "                print(\"illegal move \", move)\n",
    "            return -player_turn\n",
    "\n",
    "        board_state = apply_move(board_state, move, player_turn)\n",
    "        if log==1:\n",
    "            \n",
    "            print_board(board_state,move)\n",
    "            print(\"___________\")\n",
    "\n",
    "        if log==2:\n",
    "            \n",
    "            print(np.matrix(board_state))\n",
    "            print(\"___________\")\n",
    "\n",
    "        winner = has_winner(board_state)\n",
    "        if winner != 0:\n",
    "            if log:\n",
    "                print(\"we have a winner, side: %s\" % player_turn)\n",
    "            return winner\n",
    "        player_turn = -player_turn\n",
    "\n",
    "def play_games(number_of_games,plus_player_func, minus_player_func, log=0):\n",
    "    #plays a given number of games and prints the win rates of the players\n",
    "    results=[]\n",
    "    who_won=[0,0,0]\n",
    "    for i in range (number_of_games):\n",
    "        reward = play_game(plus_player_func, minus_player_func,log=0)\n",
    "        reward=int(reward)\n",
    "        results.append(reward)\n",
    "        who_won[reward+1]+=1\n",
    "    print(results)\n",
    "    print(\"plus player win_rate: %s\" % (who_won[2]/number_of_games))\n",
    "    print(\"minus player win_rate: %s\" % (who_won[0]/number_of_games))\n",
    "    print(\"tie rate: %s\" % (who_won[1]/number_of_games))\n",
    "\n",
    "def random_player(board_state, _):\n",
    "    \"\"\"A player func that can be used in the play_game method. Given a board state it chooses a move randomly from the\n",
    "    valid moves in the current state.\n",
    "    Args:\n",
    "        board_state (3x3 tuple of int): The current state of the board\n",
    "        _: the side this player is playing, not used in this function because we are simply choosing the moves randomly\n",
    "    Returns:\n",
    "        (int, int): the move we want to play on the current board\n",
    "    \"\"\"\n",
    "    moves = list(available_moves(board_state))\n",
    "    move = random.choice(moves)\n",
    "    return move\n",
    "\n",
    "def random_player_illegal(board_state, _):\n",
    "    \"\"\"A player func that can be used in the play_game method. Given a board state it chooses a move randomly from the\n",
    "    valid moves in the current state.\n",
    "    Args:\n",
    "        board_state (3x3 tuple of int): The current state of the board\n",
    "        _: the side this player is playing, not used in this function because we are simply choosing the moves randomly\n",
    "    Returns:\n",
    "        (int, int): the move we want to play on the current board\n",
    "    \"\"\"\n",
    "    moves = []\n",
    "    for i in range (3):\n",
    "      for j in range (3):\n",
    "        moves.append((i,j))\n",
    "    return random.choice(moves)\n",
    "\n",
    "\n",
    "\n",
    "def human_player(board_state,side):\n",
    "    moves = list(available_moves(board_state))\n",
    "    print(moves)\n",
    "    move=int(input())\n",
    "    return(moves[move])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # example of playing a game\n",
    "    play_game(policy_gradient_player,human_player,log=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#policy_gradient\n",
    "\n",
    "\n",
    "import time\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "print(tf.__version__)\n",
    "\n",
    "HIDDEN_NODES = (100, 100, 100)  # number of hidden layer neurons\n",
    "INPUT_NODES = 3 * 3  # board size\n",
    "BATCH_SIZE = 100  # every how many games to do a parameter update?\n",
    "LEARN_RATE = 1e-4\n",
    "OUTPUT_NODES = INPUT_NODES\n",
    "PRINT_RESULTS_EVERY_X = 100  # every how many games to print the results\n",
    "WEIGHTS_SAVED = False # if True starts from pre saved weights, if False starts with random weights\n",
    "ILLEGAL_MOVES_ALLOWED = True\n",
    "   \n",
    "input_placeholder = tf.compat.v1.placeholder(\"float\", shape=(None, INPUT_NODES))\n",
    "reward_placeholder = tf.compat.v1.placeholder(\"float\", shape=(None,))\n",
    "actual_move_placeholder = tf.compat.v1.placeholder(\"float\", shape=(None, OUTPUT_NODES))\n",
    "\n",
    "if (WEIGHTS_SAVED==False):\n",
    "  hidden_weights_1 = tf.Variable(tf.random.truncated_normal((INPUT_NODES, HIDDEN_NODES[0]), stddev=1. / np.sqrt(INPUT_NODES)))   #random values from a normal distribution\n",
    "  hidden_weights_2 = tf.Variable(\n",
    "      tf.random.truncated_normal((HIDDEN_NODES[0], HIDDEN_NODES[1]), stddev=1. / np.sqrt(HIDDEN_NODES[0])))\n",
    "  hidden_weights_3 = tf.Variable(\n",
    "      tf.random.truncated_normal((HIDDEN_NODES[1], HIDDEN_NODES[2]), stddev=1. / np.sqrt(HIDDEN_NODES[1]))) \n",
    "  output_weights = tf.Variable(tf.random.truncated_normal((HIDDEN_NODES[-1], OUTPUT_NODES), stddev=1. / np.sqrt(OUTPUT_NODES)))\n",
    "\n",
    "  bias_1 = tf.Variable(tf.constant(0.01, shape=(HIDDEN_NODES[0],)))\n",
    "  bias_2 = tf.Variable(tf.constant(0.01, shape=(HIDDEN_NODES[1],)))\n",
    "  bias_3 = tf.Variable(tf.constant(0.01, shape=(HIDDEN_NODES[2],)))\n",
    "  bias_4 = tf.Variable(tf.constant(0.01, shape=(OUTPUT_NODES,)))\n",
    "else:\n",
    "    %store -r saved_weights\n",
    "    hidden_weights_1 = tf.Variable(saved_weights[0])  #saved values\n",
    "    hidden_weights_2 = tf.Variable(saved_weights[1])\n",
    "    hidden_weights_3 = tf.Variable(saved_weights[2])\n",
    "    output_weights = tf.Variable(saved_weights[3])\n",
    "\n",
    "    bias_1 = tf.Variable(saved_weights[4])\n",
    "    bias_2 = tf.Variable(saved_weights[5])\n",
    "    bias_3 = tf.Variable(saved_weights[6])\n",
    "    bias_4 = tf.Variable(saved_weights[7])\n",
    "    \n",
    "    #other neural network for saved player\n",
    "    saved_input_placeholder = tf.compat.v1.placeholder(\"float\", shape=(None, INPUT_NODES))\n",
    "\n",
    "    %store -r saved_weights\n",
    "    saved_hidden_weights_1 = tf.Variable(saved_weights[0])  #saved values\n",
    "    saved_hidden_weights_2 = tf.Variable(saved_weights[1])\n",
    "    saved_hidden_weights_3 = tf.Variable(saved_weights[2])\n",
    "    saved_output_weights = tf.Variable(saved_weights[3])\n",
    "\n",
    "    saved_bias_1 = tf.Variable(saved_weights[4])\n",
    "    saved_bias_2 = tf.Variable(saved_weights[5])\n",
    "    saved_bias_3 = tf.Variable(saved_weights[6])\n",
    "    saved_bias_4 = tf.Variable(saved_weights[7])\n",
    "\n",
    "    saved_hidden_layer_1 = tf.nn.relu(tf.matmul(saved_input_placeholder, saved_hidden_weights_1) + saved_bias_1)\n",
    "    saved_hidden_layer_2 = tf.nn.relu(tf.matmul(saved_hidden_layer_1, saved_hidden_weights_2) + saved_bias_2)\n",
    "    saved_hidden_layer_3 = tf.nn.relu(tf.matmul(saved_hidden_layer_2, saved_hidden_weights_3) + saved_bias_3)\n",
    "    saved_output_layer = tf.nn.softmax(tf.matmul(saved_hidden_layer_3, saved_output_weights) + saved_bias_4)\n",
    "\n",
    "    #other neural network for saved player2\n",
    "    saved2_input_placeholder = tf.compat.v1.placeholder(\"float\", shape=(None, INPUT_NODES))\n",
    "\n",
    "    %store -r saved2_weights\n",
    "    saved2_hidden_weights_1 = tf.Variable(saved2_weights[0])  #saved values\n",
    "    saved2_hidden_weights_2 = tf.Variable(saved2_weights[1])\n",
    "    saved2_hidden_weights_3 = tf.Variable(saved2_weights[2])\n",
    "    saved2_output_weights = tf.Variable(saved2_weights[3])\n",
    "\n",
    "    saved2_bias_1 = tf.Variable(saved2_weights[4])\n",
    "    saved2_bias_2 = tf.Variable(saved2_weights[5])\n",
    "    saved2_bias_3 = tf.Variable(saved2_weights[6])\n",
    "    saved2_bias_4 = tf.Variable(saved2_weights[7])\n",
    "\n",
    "    saved2_hidden_layer_1 = tf.nn.relu(tf.matmul(saved2_input_placeholder, saved2_hidden_weights_1) + saved2_bias_1)\n",
    "    saved2_hidden_layer_2 = tf.nn.relu(tf.matmul(saved2_hidden_layer_1, saved2_hidden_weights_2) + saved2_bias_2)\n",
    "    saved2_hidden_layer_3 = tf.nn.relu(tf.matmul(saved2_hidden_layer_2, saved2_hidden_weights_3) + saved2_bias_3)\n",
    "    saved2_output_layer = tf.nn.softmax(tf.matmul(saved2_hidden_layer_3, saved2_output_weights) + saved2_bias_4)\n",
    "    \n",
    "hidden_layer_1 = tf.nn.relu(tf.matmul(input_placeholder, hidden_weights_1) + bias_1)\n",
    "hidden_layer_2 = tf.nn.relu(tf.matmul(hidden_layer_1, hidden_weights_2) + bias_2)\n",
    "hidden_layer_3 = tf.nn.relu(tf.matmul(hidden_layer_2, hidden_weights_3) + bias_3)\n",
    "output_layer = tf.nn.softmax(tf.matmul(hidden_layer_3, output_weights) + bias_4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "policy_gradient = tf.reduce_sum(input_tensor=tf.reshape(reward_placeholder, (-1, 1)) * actual_move_placeholder * output_layer)\n",
    "train_step = tf.compat.v1.train.AdamOptimizer(LEARN_RATE).minimize(-policy_gradient)\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.initialize_all_variables())\n",
    "\n",
    "\n",
    "board_states, actual_moves, rewards = [], [], []\n",
    "episode_number = 1\n",
    "results = collections.deque()\n",
    "\n",
    "\n",
    "def move_transform(move):\n",
    "# transforms a list with n^2 length into a nxn matrix\n",
    "    move_index = list(move).index(1)\n",
    "    return (move_index - (move_index % 3))/3, move_index % 3\n",
    "\n",
    "def make_move(board_state, side):\n",
    "    board_state_flat = np.ravel(board_state)\n",
    "    board_states.append(board_state_flat)\n",
    "    probability_of_actions = sess.run(output_layer, feed_dict={input_placeholder: [board_state_flat]})[0]\n",
    "\n",
    "\n",
    "\n",
    "    # we choose new moves until the move is allowed in the current board state.\n",
    "    while(True):\n",
    "\n",
    "        try:\n",
    "            move = np.random.multinomial(1, probability_of_actions)\n",
    "\n",
    "        except ValueError:\n",
    "            # sometimes because of rounding errors we end up with probability_of_actions summing to greater than 1.\n",
    "            # so need to reduce slightly to be a valid value\n",
    "            \n",
    "            move = np.random.multinomial(1, probability_of_actions / (sum(probability_of_actions) + 1e-7))\n",
    "\n",
    "        if ILLEGAL_MOVES_ALLOWED==True:\n",
    "            break\n",
    "        \n",
    "        if(move_transform(move) in (list(available_moves(board_state)))):\n",
    "          break\n",
    "        \n",
    "        # we don't allow to choose the same move again, by changing its probability to 0\n",
    "        probability_of_actions[list(move).index(1)]=0\n",
    "        if sum(probability_of_actions)==0:\n",
    "          break\n",
    "        probability_of_actions=probability_of_actions*(1/sum(probability_of_actions))\n",
    "    \n",
    "    actual_moves.append(move)\n",
    "\n",
    "\n",
    "    return move_transform(move)\n",
    "\n",
    "\n",
    "\n",
    "def policy_gradient_player(board_state, side):\n",
    "# Chooses the move with the highest probability instead of randomly choosing\n",
    "    \n",
    "    board_state_flat = np.ravel(board_state)\n",
    "    board_states.append(board_state_flat)\n",
    "    probability_of_actions = sess.run(output_layer, feed_dict={input_placeholder: [board_state_flat]})[0]\n",
    "\n",
    "    move=[0]*9\n",
    "    move[list(probability_of_actions).index(max(list(probability_of_actions)))] = 1\n",
    "\n",
    "\n",
    "    # we choose new moves until the move is allowed in the current board state.\n",
    "    while(move_transform(move) not in (list(available_moves(board_state)))):\n",
    "\n",
    "        # we don't allow to choose the same move again, by changing its probability to 0\n",
    "        probability_of_actions[list(move).index(1)]=0\n",
    "        \n",
    "        if ILLEGAL_MOVES_ALLOWED==True:\n",
    "            break\n",
    "        \n",
    "        if sum(probability_of_actions)==0:\n",
    "          break       \n",
    "        probability_of_actions=probability_of_actions*(1/sum(probability_of_actions))\n",
    "\n",
    "        move=[0]*9\n",
    "        move[list(probability_of_actions).index(max(list(probability_of_actions)))] = 1\n",
    "\n",
    "\n",
    "\n",
    "    actual_moves.append(move)\n",
    "\n",
    "\n",
    "    return move_transform(move)\n",
    "\n",
    "def saved_player(board_state, side):\n",
    "# Chooses the move with the highest probability instead of randomly choosing\n",
    "    \n",
    "    board_state_flat = np.ravel(board_state)\n",
    "    board_states.append(board_state_flat)\n",
    "    probability_of_actions = sess.run(saved_output_layer, feed_dict={saved_input_placeholder: [board_state_flat]})[0]\n",
    "\n",
    "    # we choose new moves until the move is allowed in the current board state.\n",
    "    while(True):\n",
    "\n",
    "        try:\n",
    "            move = np.random.multinomial(1, probability_of_actions)\n",
    "\n",
    "        except ValueError:\n",
    "            # sometimes because of rounding errors we end up with probability_of_actions summing to greater than 1.\n",
    "            # so need to reduce slightly to be a valid value\n",
    "            \n",
    "            move = np.random.multinomial(1, probability_of_actions / (sum(probability_of_actions) + 1e-7))\n",
    "\n",
    "        if ILLEGAL_MOVES_ALLOWED==True:\n",
    "            break\n",
    "        \n",
    "        if(move_transform(move) in (list(available_moves(board_state)))):\n",
    "          break\n",
    "        \n",
    "        # we don't allow to choose the same move again, by changing its probability to 0\n",
    "        probability_of_actions[list(move).index(1)]=0\n",
    "        if sum(probability_of_actions)==0:\n",
    "          break\n",
    "        probability_of_actions=probability_of_actions*(1/sum(probability_of_actions))\n",
    "    \n",
    "    actual_moves.append(move)\n",
    "\n",
    "\n",
    "    return move_transform(move)\n",
    "\n",
    "def saved2_player(board_state, side):\n",
    "# Chooses the move with the highest probability instead of randomly choosing\n",
    "    \n",
    "    board_state_flat = np.ravel(board_state)\n",
    "    board_states.append(board_state_flat)\n",
    "    probability_of_actions = sess.run(saved2_output_layer, feed_dict={saved2_input_placeholder: [board_state_flat]})[0]\n",
    "\n",
    "    # we choose new moves until the move is allowed in the current board state.\n",
    "    while(True):\n",
    "\n",
    "        try:\n",
    "            move = np.random.multinomial(1, probability_of_actions)\n",
    "\n",
    "        except ValueError:\n",
    "            # sometimes because of rounding errors we end up with probability_of_actions summing to greater than 1.\n",
    "            # so need to reduce slightly to be a valid value\n",
    "            \n",
    "            move = np.random.multinomial(1, probability_of_actions / (sum(probability_of_actions) + 1e-7))\n",
    "\n",
    "        if ILLEGAL_MOVES_ALLOWED==True:\n",
    "            break\n",
    "        \n",
    "        if(move_transform(move) in (list(available_moves(board_state)))):\n",
    "          break\n",
    "        \n",
    "        # we don't allow to choose the same move again, by changing its probability to 0\n",
    "        probability_of_actions[list(move).index(1)]=0\n",
    "        if sum(probability_of_actions)==0:\n",
    "          break\n",
    "        probability_of_actions=probability_of_actions*(1/sum(probability_of_actions))\n",
    "    \n",
    "    actual_moves.append(move)\n",
    "\n",
    "\n",
    "    return move_transform(move)\n",
    "\n",
    "\n",
    "\n",
    "win_rates=[]\n",
    "seconds = time.time()\n",
    "\n",
    "while True:\n",
    "    reward = play_game(make_move ,random_player)\n",
    "    results.append(reward)\n",
    "    if len(results) > PRINT_RESULTS_EVERY_X:\n",
    "        results.popleft()\n",
    "\n",
    "    last_game_length = len(board_states) - len(rewards)\n",
    "\n",
    "    # we scale here so winning quickly is better winning slowly and loosing slowly better than loosing quick\n",
    "    reward /= float(last_game_length)\n",
    "\n",
    "    rewards += ([reward] * last_game_length)\n",
    "\n",
    "    episode_number += 1\n",
    "\n",
    "    if episode_number % BATCH_SIZE == 0:\n",
    "        normalized_rewards = rewards - np.mean(rewards)\n",
    "        if np.std(normalized_rewards)!=0:\n",
    "            normalized_rewards /= np.std(normalized_rewards)\n",
    "\n",
    "        \n",
    "            sess.run(train_step, feed_dict={input_placeholder: board_states,\n",
    "                                        reward_placeholder: normalized_rewards,\n",
    "                                        actual_move_placeholder: actual_moves})\n",
    "\n",
    "            # clear batches\n",
    "            del board_states[:]\n",
    "            del actual_moves[:]\n",
    "            del rewards[:]\n",
    "\n",
    "\n",
    "    if episode_number % PRINT_RESULTS_EVERY_X == 0:\n",
    "        print(\"episode: %s win_rate: %s                                     Time elapsed : %s\" % (episode_number, results.count(1) / (PRINT_RESULTS_EVERY_X), round(time.time()-seconds,3) ))\n",
    "        seconds = time.time()\n",
    "        win_rates.append((results.count(1) / (PRINT_RESULTS_EVERY_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play a number of gaves and print winrates\n",
    "ILLEGAL_MOVES_ALLOWED = False\n",
    "play_games(1000,make_move, random_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random_player_win_rates = [0.5860]*len(win_rates)\n",
    "random_player_illegal_win_rates = [0.08479]*len(win_rates)\n",
    "\n",
    "f = plt.figure() \n",
    "f.set_figwidth(8) \n",
    "f.set_figheight(5) \n",
    "\n",
    "plt.plot(win_rates,  label=\"Policy gradient játékos\")\n",
    "plt.plot(random_player_win_rates, 'g', label=\"Véletlen játékos\")\n",
    "plt.plot(random_player_illegal_win_rates,'r', label=\"Szabálytalan véletlen\")\n",
    "plt.ylabel('győzelmi arány')\n",
    "plt.xlabel('játékok száma (száz játék)')\n",
    "legend=plt.legend(loc='best')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def _score_line(line):\n",
    "    minus_count = line.count(-1)\n",
    "    plus_count = line.count(1)\n",
    "    if minus_count + plus_count < 3:\n",
    "        if minus_count == 2:\n",
    "            return -1\n",
    "        elif plus_count == 2:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def evaluate(board_state):\n",
    "    \"\"\"Get a rough score for how good we think this board position is for the plus_player. Does this based on number of\n",
    "    2 in row lines we have.\n",
    "    Args:\n",
    "        board_state (3x3 tuple of int): The board state we are evaluating\n",
    "    Returns:\n",
    "        int: evaluated score for the position for the plus player, posative is good for the plus player, negative good\n",
    "            for the minus player\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    for x in range(3):\n",
    "        score += _score_line(board_state[x])\n",
    "    for y in range(3):\n",
    "        score += _score_line([i[y] for i in board_state])\n",
    "\n",
    "    # diagonals\n",
    "    score += _score_line([board_state[i][i] for i in range(3)])\n",
    "    score += _score_line([board_state[2 - i][i] for i in range(3)])\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def min_max(board_state, side, max_depth, evaluation_func=evaluate):\n",
    "    \"\"\"Runs the min_max_algorithm on a given board_sate for a given side, to a given depth in order to find the best\n",
    "    move\n",
    "    Args:\n",
    "        board_state (3x3 tuple of int): The board state we are evaluating\n",
    "        side (int): either +1 or -1\n",
    "        max_depth (int): how deep we want our tree to go before we use the evaluate method to determine how good the\n",
    "        position is.\n",
    "        evaluation_func (board_state -> int): Function used to evaluate the position for the plus player\n",
    "    Returns:\n",
    "        (best_score(int), best_score_move((int, int)): the move found to be best and what it's min-max score was\n",
    "    \"\"\"\n",
    "\n",
    "    best_score = None\n",
    "    best_score_move = None\n",
    "\n",
    "    moves = list(available_moves(board_state))\n",
    "    if not moves:\n",
    "        # this is a draw\n",
    "        return 0, None\n",
    "\n",
    "    for move in moves:\n",
    "        new_board_state = apply_move(board_state, move, side)\n",
    "        winner = has_winner(new_board_state)\n",
    "        if winner != 0:\n",
    "            return winner * 10000, move\n",
    "        else:\n",
    "            if max_depth <= 1:\n",
    "                score = evaluation_func(new_board_state)\n",
    "            else:\n",
    "                score, _ = min_max(new_board_state, -side, max_depth - 1)\n",
    "            if side > 0:\n",
    "                if best_score is None or score > best_score:\n",
    "                    best_score = score\n",
    "                    best_score_move = move\n",
    "            else:\n",
    "                if best_score is None or score < best_score:\n",
    "                    best_score = score\n",
    "                    best_score_move = move\n",
    "\n",
    "    return best_score, best_score_move\n",
    "\n",
    "\n",
    "def min_max_alpha_beta(board_state, side, max_depth, evaluation_func=evaluate, alpha=-sys.float_info.max,\n",
    "                       beta=sys.float_info.max):\n",
    "    \"\"\"Runs the min_max_algorithm on a given board_sate for a given side, to a given depth in order to find the best\n",
    "    move\n",
    "    Args:\n",
    "        board_state (3x3 tuple of int): The board state we are evaluating\n",
    "        side (int): either +1 or -1\n",
    "        max_depth (int): how deep we want our tree to go before we use the evaluate method to determine how good the\n",
    "        position is.\n",
    "        evaluation_func (board_state -> int): Function used to evaluate the position for the plus player\n",
    "        alpha (float): Used when this is called recursively, normally ignore\n",
    "        beta (float): Used when this is called recursively, normally ignore\n",
    "    Returns:\n",
    "        (best_score(int), best_score_move((int, int)): the move found to be best and what it's min-max score was\n",
    "    \"\"\"\n",
    "    best_score_move = None\n",
    "    moves = list(available_moves(board_state))\n",
    "    if not moves:\n",
    "        return 0, None\n",
    "\n",
    "    for move in moves:\n",
    "        new_board_state = apply_move(board_state, move, side)\n",
    "        winner = has_winner(new_board_state)\n",
    "        if winner != 0:\n",
    "            return winner * 10000, move\n",
    "        else:\n",
    "            if max_depth <= 1:\n",
    "                score = evaluation_func(new_board_state)\n",
    "            else:\n",
    "                score, _ = min_max_alpha_beta(new_board_state, -side, max_depth - 1, evaluation_func, alpha, beta)\n",
    "\n",
    "        if side > 0:\n",
    "            if score > alpha:\n",
    "                alpha = score\n",
    "                best_score_move = move\n",
    "        else:\n",
    "            if score < beta:\n",
    "                beta = score\n",
    "                best_score_move = move\n",
    "        if alpha >= beta:\n",
    "            break\n",
    "\n",
    "    return alpha if side > 0 else beta, best_score_move\n",
    "\n",
    "\n",
    "def min_max_player(board_state, side):\n",
    "    return min_max(board_state, side, 10000)[1]\n",
    "\n",
    "def min_max_player_alpha_beta(board_state, side):\n",
    "    return min_max_alpha_beta(board_state, side, 10000)[1]\n",
    "\n",
    "def min_max_player_alpha_beta2(board_state, side):\n",
    "    return min_max_alpha_beta(board_state, side, 3)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "ILLEGAL_MOVES_ALLOWED=False\n",
    "play_game(make_move, random_player, log=2)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the weights and biases\n",
    "\n",
    "saved_weights=[]\n",
    "\n",
    "def save_weights(hidden_weights_1,hidden_weights_2,hidden_weights_3,output_weights,bias_1,bias_2,bias_3,bias_4):\n",
    "  weights = []\n",
    "  weights.append(sess.run(hidden_weights_1))\n",
    "  weights.append(sess.run(hidden_weights_2))\n",
    "  weights.append(sess.run(hidden_weights_3))\n",
    "  weights.append(sess.run(output_weights))\n",
    "  weights.append(sess.run(bias_1))\n",
    "  weights.append(sess.run(bias_2))\n",
    "  weights.append(sess.run(bias_3))\n",
    "  weights.append(sess.run(bias_4))\n",
    "  return weights\n",
    "\n",
    "saved_weights=save_weights(hidden_weights_1,hidden_weights_2,hidden_weights_3,output_weights,bias_1,bias_2,bias_3,bias_4)\n",
    "\n",
    "%store saved_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the weights and biases 2\n",
    "\n",
    "saved2_weights=[]\n",
    "\n",
    "def save_weights(hidden_weights_1,hidden_weights_2,hidden_weights_3,output_weights,bias_1,bias_2,bias_3,bias_4):\n",
    "  weights = []\n",
    "  weights.append(sess.run(hidden_weights_1))\n",
    "  weights.append(sess.run(hidden_weights_2))\n",
    "  weights.append(sess.run(hidden_weights_3))\n",
    "  weights.append(sess.run(output_weights))\n",
    "  weights.append(sess.run(bias_1))\n",
    "  weights.append(sess.run(bias_2))\n",
    "  weights.append(sess.run(bias_3))\n",
    "  weights.append(sess.run(bias_4))\n",
    "  return weights\n",
    "\n",
    "saved2_weights=save_weights(hidden_weights_1,hidden_weights_2,hidden_weights_3,output_weights,bias_1,bias_2,bias_3,bias_4)\n",
    "\n",
    "%store saved2_weights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
